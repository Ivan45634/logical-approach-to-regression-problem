\input{config}
\documentclass[10pt,pdf,unicode,aspectratio=169]{beamer}
\mode<presentation>{\usetheme{Lankton}}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{booktabs, comment} 
\usepackage[absolute, overlay]{textpos} 
\usepackage{pgfpages}
\usepackage[font=footnotesize]{caption}
\usepackage[russian]{babel}
\usepackage[T1]{fontenc}
\usepackage[13pt]{extsizes}

%\usepackage[orientation=portrait,size=custom,width=40,height=80,scale=.6,debug]{beamerposter}

\usepackage{ragged2e}
%\usepackage{beamerposter}
%[orientation=portrait,size=a4,scale=0.5]

\usepackage{natbib}
\bibliographystyle{<стиль>}
\bibliography{<имя_файла>}

% other packages
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}

\useoutertheme{infolines}

\usetheme{boxes} % Выбор темы презентации
%\usecolortheme{lily}
%\setbeamertemplate{navigation symbols}{}

\hyphenpenalty=100000

%Настройка шрифтов
\input{styles/fonts}
%Обеспечиваем работу с математикой
\input{styles/math}
%Настраиваем дизайн текста (отступы, интервалы и пр.)
\input{styles/text}
%Настройка дизайна страницы
 \input{styles/page}
%Обеспечиваем работу с медиа-контентом (с рисунками)
\input{styles/media}
%Обеспечиваем работу с таблицами
\input{styles/table}
%Обеспечиваем оформление алгоритмов и листингов
\input{styles/algorithms}

%\input{virens_beamerposter_demo}

\usepackage[% 
  backend=biber, %подключение пакета biber
  bibstyle=gost-numeric, %подключение одного из четырех главных стилей biblatex-gost 
  citestyle=numeric-comp, %подключение стиля 
  language=auto, %указание сортировки языков
  sorting=none, %тип сортировки в библиографии
  % doi=false, 
  % eprint=false, 
  % isbn=false, 
  % dashed=false, 
  % url=false
]{biblatex}
\addbibresource{slides.bib}

\title{О логическом подходе в задаче восстановления регрессии} % Заголовок презентации
\author{Листопадов Иван Сергеевич} % Автор презентации
%\institute[]{Факультет Вычислительной Математики и Кибернетики  \\Московский государственный университет имени М. В. Ломоносова} % Институт
\date{11 октября 2023 г.} % Дата проведения презентации

\institute[\UniverAbbr] % (optional, but mostly needed)
{
  \Univer \\
  \Faculty \\
  \Department 
}

\AtBeginSubsection[]
{
  \begin{frame}<beamer>
    \frametitle{Содержание}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}


\begin{document}

\pgfdeclareimage[height=0.7cm]{university-logo}{cmclogo.png}
\logo{\pgfuseimage{university-logo}}

\frame{\titlepage} % Титульный слайд


\begin{frame}{Основные понятия}

\begin{itemize}
  \item $M$ - множество объектов;

  \item $\left\{x_{1}, \ldots, x_{n}\right\}$ - множество признаков;

  \item $\left(x_{1}(S), \ldots, x_{n}(S)\right)$ - признаковое описание объекта $S \in M$;

  \item $Y$ - множество ответов;

  \item $y: M \rightarrow Y$ - целевая функция;

  \item $T=\left\{S_{1}, \ldots, S_{m}\right\}$ - обучающая выборка (прецеденты);

  \item $y_{i}=y\left(S_{i}\right)$ - значения целевой функции на прецеденте $S_{i}$;

  \item $A_{T}: M \rightarrow Y$ - алгоритм распознавания;

  \item $H$ - набор различных признаков;

  \item $H(S)$ - признаковое подописание объекта $S$, определяемое набором признаков $H$.
\end{itemize}

\end{frame}

\begin{frame}{Постановка задачи}
\textbf{Дано:}

$T=\left\{S_{1}, \ldots, S_{m}\right\}, y_{i}=y\left(S_{i}\right), i=1,..m$.

\textbf{Найти:}\\ Некоторую вещественнозначную величину, т.е. $Y = \mathbb{R}$

\textbf{Критерий:}\\ Алгоритм $A_{T}: Q(A_{T}, T) \rightarrow min$, т.е. необходимо построить алгоритм который минимизирует функционал ошибки и наилучшим образом приближает зависимость между признаковыми описаниями объектов $T$ и значениями целевой переменной $Y$.
\end{frame}

\begin{frame}
\frametitle{Линейная регрессия. Сведение к оптимизационной задаче}
\begin{enumerate}
    \item 
    Для описания зависимости целевой переменной $Y$ от признаковов $x_{1}, \ldots, x_{n}$ используется линейная модель:
    $$
    Y=\beta_{0}+\beta_{1} x_{1}+\ldots+\beta_{n} x_{n},
    $$
    \item Метод обучения - метод наименьших квадратов (МНК):
    $$
    \sum_{j=1}^{m}\left[y_{j}-\beta_{0}-\sum_{i=1}^{n} x_{i} \beta_{i}\right]^{2}
    $$
    \item Проверка по тестовой выборке $X^k$:
        $$
        Q(X^k) = \frac{1}{k}\sum_{i=1}^k \left[\widetilde{y_{j}}-\beta_{0}-\sum_{i=1}^{n} \widetilde{x_{i}} \beta_{i}\right]^{2} 
        $$
\end{enumerate}


\end{frame}


\begin{frame}{Метрические методы регрессии (kNN-регрессия)}
\textbf{Обучение:}
\[
        d(.,.) \text{-- функция расстояния, например, евклидово расстояние, тогда}
\]
\[
    d(S_k, S_p) = \sqrt{\sum_{j=1}^{n} (x_{j}(S_k) - x_{j}(S_p)^2}.
\]

\textbf{Отладка (подбор гиперпараметра k):}

\[
\text{Выбор оптимального } k: k^* = \arg\min_{k} \frac{1}{m} \sum_{i=1}^{m} \left( y_i - \frac{1}{k} \sum_{i=1}^{k} y_{\text{сосед}} \right)^2.
\]

\textbf{Прогнозирование:}
\[
\text{Найти } k \text{ ближайших соседей для } S_0:
d(S_0, S_0^1) \leqslant d(S_0, S_0^2) \leqslant ... \leqslant d(S_0, S_0^k);
\]
\[
\text{Прогноз: } \hat{y} = \frac{1}{k} \sum_{i=1}^{k} y_{\text{сосед}}
\]
\end{frame}

\begin{frame}
\frametitle{Предложенный алгоритм: A1-Rg}
  
  \[\textbf{Разметка обучающего набора данных с помощью кластеризации} \]
  \begin{itemize}
% \item Кластеризация обучающих объектов:
      \begin{equation}
        \mathcal{Y} = \text{DBSCAN}(X, \epsilon)
      \end{equation}
  \end{itemize}
  \[\textbf{Классификация по прецедентам}\]
  \begin{itemize}
    \item Поиск тупиковых представительных элементарных классификаторов:
      \begin{equation}
        h_i = \arg\min_h\ \sum_{y_j \neq y_i} w_j \cdot I(h(x_j) \neq y_j)
      \end{equation}
  \end{itemize}
  \[\textbf{Прогнозирование}\]
  \begin{itemize}
    \item Восстановление значения целевой переменной:
      \begin{equation}
        y_{new} = \frac{\sum_{x_j \in C_{new}} y_j \cdot w_j}{\sum_{x_j \in C_{new}} w_j}
      \end{equation}
  \end{itemize}
\end{frame}


\begin{frame}{Описание экспериментальной части}

\begin{itemize}
    \item \textbf{Цель экспериментов} - сравнить алгоритм A1-Rg с классическими методами восстановления регрессии:
    \begin{itemize}
        \item Линейная регрессия (LR)
        \item kNN-регрессия (kNR)
        \item Градиентный бустинг (GB)
        \item Регрессия с помощью метода опорных векторов (SVR)
        \item Случайный лес (RF)
    \end{itemize}
    \item \textbf{Предобработка данных}:
    \begin{itemize}
        \item Кодирование вещественных признаков в категориальные
        \item Удаление колонок с большим количеством уникальных значений
        \item Удаление объектов с пропущенными значениями
        \item Кодирование категориальных признаков в числовые значения
    \end{itemize}
    \item \textbf{Оценка моделей}:
    Кросс-валидация с 5 фолдами,
    метрики оценки: RMSE и коэффициент детерминации R2,
    10 независимых запусков для более точной оценки.
\end{itemize}

\end{frame}

% \begin{frame}
% \frametitle{Эксперименты на реальных данных}
% \begin{figure}
%                 \centering
%                 \begin{subfigure}{\textwidth}
%                     \includegraphics[width=\linewidth]{res1.png}
%                 \end{subfigure}
% \end{figure}
%                 Таблица 1: Качество работы алгоритмов (MSE)

% \end{frame}

\begin{frame}
\frametitle{Результаты экспериментов}
\begin{figure}
                \centering
                \begin{subfigure}{\textwidth}
                    \includegraphics[width=\linewidth]{res2.png}
                \end{subfigure}
\end{figure}
                Таблица 1: Качество работы алгоритмов (R2-Score)

\end{frame}

% \begin{frame}
% \frametitle{Эксперименты на реальных данных}
% \begin{figure}
%                 \centering
%                 \begin{subfigure}{\textwidth}
%                     \includegraphics[width=\linewidth]{res3.png}
%                 \end{subfigure}
% \end{figure}
                

% \end{frame}

\begin{frame}[fragile]{Ансамбль моделей. Бустинг}

\textbf{Инициализация:}
\begin{itemize}
  \item Обучающая выборка - $T$, количество базовых моделей - $N$, $F_0(x) = \text{const}$ - начальное приближение ответа.
\end{itemize}

\textbf{Идея - обучение по остаткам:}
\begin{enumerate}
  \item На $n$-ом шаге вычисляется остаток на предыдущей итерации:
    \[
    r_{n-1}(x_i) = y_i - F_{n-1}(x_i)
    \]
  \item Строится базовая модель $b_n(x)$, которая описывает остаток:
    \[
    b_n(x) = \text{arg}\min_{b} \sum_{i=1}^{m} L(r_{n-1}(x_i), b(x_i))
    \]
   \item К базовой модели добавляется с весом $\alpha_n$, формируя композицию:
    \[
    F_n(x) = F_{n-1}(x) + \alpha_n b_n(x)
    \]
  \item Обновляется ответ:
    \[
    y_i = F_n(x_i)
    \]
\end{enumerate}

% \textbf{Итоговый ответ:}
% \[
% F_N(x) = \sum_{n=1}^{N} \alpha_n b_n(x)
% \]

\end{frame}

\begin{frame}
\frametitle{Предложенный алгоритм: бустинг над эл.кл.}
\begin{enumerate}
    \item На каждой итерации (t) строим слабый алгоритм $h_t$ вычисляя остатки при решении задачи регрессии:

   \[h_t = \arg\min_h \sum_{S_{l_j} \in D} [H(S_{i_t})=H(S_{l_j})] \cdot (y_{i_t} - y_{l_j})^2\]
   \item Вычисляем веса \(\alpha_t\) для слабого алгоритма \(h_t\):

   \[\alpha_t = \arg\min_{\alpha} \sum_{i \neq i_t} \left| y_{i_t} - y_i \right|\]
   \item Обновляем остатки, учитывая вклад каждого слабого алгоритма:
   \[r_i = y_i - \sum_{t=1}^{T} \alpha_t \cdot h_t(x_i)\]
   \item Выбираем объекты с наибольшими остатками \(r_i\) для построения следующего слабого алгоритма:
   \[S_i = \{x_i\}_{i=1}^{r}, \text{ где } r \text{ - параметр алгоритма}\]

   
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Бустинг над эл.кл. (продолжение)}
\begin{enumerate}
    \setcounter{enumi}{4}
    \item Построение булевой матрицы сравнения \(L_t\) для итерации \(t\):
    $$
    L_{t}=\left\|a_{l j}\right\|, i \in\{1, \ldots, r\}, j \in\{1, \ldots, n\} \text {, }
    $$
    в которой
    $$
    a_{l j}=\left[x_{j}\left(S_{k_{l}}\right) \neq x_{j}\left(S_{i_{t}}\right)\right],
    $$
    то есть в каждой строке $l$ индикаторы того, по каким признаком можно различить объекты $S_{k_{l}}$ и $S_{i_{t}}$
    \item Нахождение неприводимого покрытия \(H_t\) матрицы \(L_t\) с использованием квадратичного функционала.
    \item Слабый алгоритм \(h_t\) использует набор признаков \(H_t\) и объект \(x_i\) для построения предсказания:
   \[h_t(x_i) = [H(S_{i_t}) = H(S_{l_j})] \cdot f(y_{i_t})\]
\end{enumerate}
\end{frame}

% 1. На каждой итерации (t) строим слабый алгоритм h_t вычисляя остатки при решении задачи регрессии:

%    \[h_t = \arg\min_h \sum_{S_{l_j} \in D} [H(S_{i_t})=H(S_{l_j})] \cdot (y_{i_t} - y_{l_j})^2\]

% 2. Вычисляем веса \(\alpha_t\) для слабого алгоритма \(h_t\):

%    \(\alpha_t = \arg\min_{\alpha} \sum_{i \neq i_t} \left| y_{i_t} - y_i \right|\)
% 3. Обновляем остатки, учитывая вклад каждого слабого алгоритма:
%    \[r_i = y_i - \sum_{t=1}^{T} \alpha_t \cdot h_t(x_i)\]

% 4. Выбираем объекты с наибольшими остатками \(r_i\) для построения следующего слабого алгоритма:
%    \[S_i = \{x_i\}_{i=1}^{r}, \text{ где } r \text{ - параметр алгоритма}\]


% 7. Слабый алгоритм \(h_t\) использует набор признаков \(H_t\) и объект \(x_i\) для построения предсказания:
%    \[h_t(x_i) = [H(S_{i_t}) = H(S_{l_j})] \cdot f(y_{i_t})\]

% 8. Повторяем шаги 2-7 до достижения критерия останова.



\begin{frame}{Слайд о будущей работе}
\bigskip
Исследовать методы решения задачи восстановления регрессии с применением дискретных процедур.

\begin{block}{Цель работы~---}
Построить оптимальный алгоритм восстановления регрессии с точки зрения выбранной метрики качества на базе дискретных процедур распознавания и экспериментально сравнить различные подходы.
\end{block}
\begin{block}{Необходимо реализовать}
Алгоритм бустинга над элементарными классификаторами с использованием описанной схемы работы.
\end{block}

\end{frame}

\end{document}